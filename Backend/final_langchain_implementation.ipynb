{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7604cde-7aa5-4197-9280-dbe5283617ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.chains import LLMChain\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, ChatSession\n",
    "import re\n",
    "import time\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca2df48-3e00-4bc6-bb76-9b5ce5eba6df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "`VertexAI` is not fully defined; you should define `_LanguageModel`, then call `VertexAI.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/class-not-fully-defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m vertexai\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madsp-capstone-once-upon\u001b[39m\u001b[38;5;124m'\u001b[39m, location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mus-central1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the LLM\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mVertexAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-1.0-pro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the text summarization pipeline\u001b[39;00m\n\u001b[0;32m      8\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Christian\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Christian\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\load\\serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Christian\\anaconda3\\envs\\llm\\Lib\\site-packages\\pydantic\\_internal\\_mock_val_ser.py:99\u001b[0m, in \u001b[0;36mMockValSer.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# raise an AttributeError if `item` doesn't exist\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_or_ser, item)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message, code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code)\n",
      "\u001b[1;31mPydanticUserError\u001b[0m: `VertexAI` is not fully defined; you should define `_LanguageModel`, then call `VertexAI.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/class-not-fully-defined"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI\n",
    "vertexai.init(project='adsp-capstone-once-upon', location = 'us-central1')\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = VertexAI(model_name=\"gemini-1.0-pro\", max_output_tokens=8192)\n",
    "\n",
    "# Initialize the text summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2669c207-e429-40af-a757-de91c3f3d61e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian\\AppData\\Local\\Temp\\ipykernel_10552\\3265483459.py:30: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=<vertexai.preview.generat...t at 0x000001E2F58DB150>, input_type=GenerativeModel]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=<vertexai.preview.generat...t at 0x000001E2F58DB150>, input_type=GenerativeModel]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m\n\u001b[0;32m      2\u001b[0m summary_prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m      3\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_age\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      4\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Create LLMChain for summary generation\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m summary_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_prompt_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_summary\u001b[39m(input_prompt, user_age):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Christian\\anaconda3\\envs\\googlellm\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Christian\\anaconda3\\envs\\googlellm\\Lib\\site-packages\\langchain_core\\load\\serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Christian\\anaconda3\\envs\\googlellm\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=<vertexai.preview.generat...t at 0x000001E2F58DB150>, input_type=GenerativeModel]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=<vertexai.preview.generat...t at 0x000001E2F58DB150>, input_type=GenerativeModel]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of"
     ]
    }
   ],
   "source": [
    "# Define prompt template for the summary\n",
    "summary_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input_prompt\", \"user_age\"],\n",
    "    template=\"\"\"\n",
    "    Generate a complete summary for a 3-chapter story that is engaging for a {user_age}-year-old child. The story should be about: {input_prompt}\n",
    "\n",
    "    Your response must include:\n",
    "    1. A brief introduction to the story\n",
    "    2. Chapter 1 summary (at least 100 words)\n",
    "    3. Chapter 2 summary (at least 100 words)\n",
    "    4. Chapter 3 summary (at least 100 words)\n",
    "\n",
    "    Format your response as follows:\n",
    "    Story Introduction: [Your introduction here]\n",
    "\n",
    "    Chapter 1: [Title]\n",
    "    [Chapter 1 summary]\n",
    "\n",
    "    Chapter 2: [Title]\n",
    "    [Chapter 2 summary]\n",
    "\n",
    "    Chapter 3: [Title]\n",
    "    [Chapter 3 summary]\n",
    "\n",
    "    Ensure each chapter summary is complete and shows a clear progression of the story.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create LLMChain for summary generation\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template)\n",
    "\n",
    "def generate_summary(input_prompt, user_age):\n",
    "    while True:\n",
    "        summary = summary_chain.run(input_prompt=input_prompt, user_age=user_age)\n",
    "        print(\"Complete Story Summary:\")\n",
    "        print(summary)\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca96a3d-a82a-4487-8ba3-64c7628f1d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt template for chapter generation\n",
    "#Input needs to include previous chapter summary, theme, character\n",
    "chapter_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"chapter_summary\", \"chapter_number\", \"user_age\"],\n",
    "    template=\"\"\"\n",
    "    Based on this chapter summary for a {user_age}-year-old child:\n",
    "    {chapter_summary}\n",
    "\n",
    "    Generate a detailed Chapter {chapter_number} of AT LEAST 1000 WORDS. The chapter should be written in paragraphs, not bullet points, and should be engaging for a {user_age}-year-old child.\n",
    "\n",
    "    Do not include any additional notes, just the story that the audience should read. Ensure the chapter matches the summary while expanding on the details and dialog.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create LLMChain for chapter generation\n",
    "chapter_chain = LLMChain(llm=llm, prompt=chapter_prompt_template)\n",
    "\n",
    "def generate_chapter(chapter_summary, chapter_number, user_age):\n",
    "    chapter = chapter_chain.run(chapter_summary=chapter_summary, chapter_number=chapter_number, user_age=user_age)\n",
    "    print(f\"\\nChapter {chapter_number} Generated:\")\n",
    "    print(chapter)\n",
    "    return chapter\n",
    "\n",
    "def generate_story_in_chapters(summary_object, user_age):\n",
    "    chapters = re.split(r'Chapter \\d+:', summary_object)[1:]\n",
    "    generated_chapters = []\n",
    "\n",
    "    for i, chapter_summary in enumerate(chapters, 1):\n",
    "        chapter = generate_chapter(chapter_summary, i, user_age)\n",
    "        generated_chapters.append(chapter)\n",
    "\n",
    "    return generated_chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8ca57c-6bfd-438a-be0d-69d23c59e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the age of the child (e.g., 5):  7\n",
      "What would you like the story to be about?  A story about Ada Lovelace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Story Summary:\n",
      "## Story Introduction:\n",
      "\n",
      "Ada Lovelace was a brilliant young girl who loved numbers and stories. She dreamed of creating a machine that could tell stories using numbers, a machine that could do much more than just add and subtract. Join Ada on her journey as she explores the world of mathematics and invents the first computer program!\n",
      "\n",
      "## Chapter 1: A Curious Mind\n",
      "\n",
      "Ada was born into a world of books and ideas. Her mother, a famous writer, encouraged Ada's love for learning. Ada spent hours reading about science, math, and even poetry. But what fascinated her most were the complex machines her father, a renowned inventor, was building. Ada would watch him for hours, her mind buzzing with questions about how they worked. One day, she asked her father, \"Can a machine ever think like a human?\" Her father smiled and said, \"Perhaps, Ada. Perhaps.\" This sparked a fire in Ada's heart. She was determined to find out.\n",
      "\n",
      "## Chapter 2: Numbers and Stories\n",
      "\n",
      "Ada began studying mathematics with a passion. She learned about equations, algorithms, and the power of numbers. But she never forgot her love for stories. She imagined numbers dancing on paper, telling tales of adventure and wonder. One day, she came across a new invention called the Analytical Engine, a machine designed to perform complex calculations. Ada saw the potential in this machine. She realized that it could be used to tell stories, not just with words, but with numbers!\n",
      "\n",
      "## Chapter 3: The First Program\n",
      "\n",
      "Ada worked tirelessly, translating stories into numbers and feeding them into the Analytical Engine. She created the first computer program, a set of instructions that the machine could follow to perform a specific task. It was a groundbreaking achievement, a testament to Ada's brilliance and imagination. Ada's program paved the way for the modern computer, a machine that has revolutionized the world. Though she never saw her invention come to life, Ada's legacy lives on, inspiring generations of scientists, mathematicians, and dreamers to push the boundaries of what is possible. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to continue with this summary? (YES/NO):  YES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chapter 1 Generated:\n",
      "## Chapter 1: A World of Wonder\n",
      "\n",
      "Ada Lovelace was born into a world that sparkled with stories and ideas. Her mother, the renowned writer Lady Byron, filled their home with books that whispered tales of faraway lands and brave heroes. Ada, with her bright eyes and insatiable curiosity, devoured these stories, her imagination taking flight with each turn of the page.\n",
      "\n",
      "But books weren't the only things that captivated Ada. Her father, Lord Byron, was a brilliant inventor, his mind constantly churning with new contraptions and machines. Ada would spend hours in his workshop, watching him tinker and build, her mind buzzing with questions about how these wondrous creations worked.\n",
      "\n",
      "One day, as Lord Byron was assembling a complex clockwork automaton, Ada, then just five years old, tugged at his sleeve. \"Papa,\" she asked, her voice filled with wonder, \"can a machine ever think like a human?\"\n",
      "\n",
      "Lord Byron paused, his eyes twinkling with amusement. \"Perhaps, Ada,\" he said with a smile. \"Perhaps.\"\n",
      "\n",
      "Those two words, spoken with such conviction, ignited a spark within Ada. From that moment on, she was determined to find out. She devoured books on mathematics and science, her mind hungry for knowledge. She spent countless hours in her father's workshop, observing, questioning, and learning everything she could about the intricate workings of machines.\n",
      "\n",
      "One afternoon, while exploring the library, Ada stumbled upon a book that would change her life forever. It was a treatise on a new invention called the Analytical Engine, a machine unlike any other. This engine, designed by the brilliant inventor Charles Babbage, was not just a calculator; it was a machine capable of performing complex calculations and even composing music.\n",
      "\n",
      "Ada was mesmerized. She spent days poring over the book, studying the diagrams and descriptions of the Analytical Engine. She realized that this machine, with its gears and wheels and intricate calculations, held the key to unlocking the secrets of how machines could think.\n",
      "\n",
      "Fueled by her newfound passion, Ada began to experiment. She started by creating simple programs for the Analytical Engine, using its gears and wheels to perform basic calculations. As she progressed, her programs grew more complex, tackling intricate mathematical problems that would have taken humans hours to solve.\n",
      "\n",
      "One day, while working on a particularly challenging program, Ada had a breakthrough. She realized that the Analytical Engine could be used to do more than just calculations. It could be used to manipulate symbols, to create patterns, to even write music.\n",
      "\n",
      "This realization filled Ada with excitement. She saw the potential of the Analytical Engine to revolutionize the world, to solve complex problems, and to create beautiful works of art. She knew that she had to share her discoveries with the world.\n",
      "\n",
      "And so, Ada Lovelace, the young girl who had once asked if machines could think like humans, became the world's first computer programmer. She wrote the first algorithms for the Analytical Engine, laying the foundation for the modern computer age.\n",
      "\n",
      "Ada's story is a testament to the power of curiosity and the importance of pursuing your dreams. It is a reminder that even the most complex machines can be understood and even controlled by the human mind. And it is a testament to the enduring legacy of a young girl who dared to ask the question, \"Can a machine ever think like a human?\"\n",
      "\n",
      "Chapter 2 Generated:\n",
      "## Chapter 2: The Numbers Come Alive\n",
      "\n",
      "Ada's eyes sparkled with excitement as she flipped through the pages of a book about the Analytical Engine. The intricate diagrams and descriptions of gears, cogs, and levers filled her with a sense of wonder. This machine, unlike anything she had ever seen, promised to unlock the secrets of numbers and their hidden stories.\n",
      "\n",
      "\"Imagine, Ada,\" her father, Lord Byron, said, his voice filled with pride, \"this machine can solve problems faster than any human ever could. It can calculate the movement of stars, predict the weather, and even compose music!\"\n",
      "\n",
      "Ada's mind raced with possibilities. Numbers, which had always been a source of fascination for her, now seemed to possess a magical quality. They weren't just symbols on a page; they were the building blocks of a universe waiting to be explored.\n",
      "\n",
      "One afternoon, while exploring the vast library of her home, Ada stumbled upon a dusty old book titled \"The Adventures of Pinocchio.\" As she read the story of the wooden puppet who longed to become a real boy, an idea sparked in her mind.\n",
      "\n",
      "\"What if,\" she thought, \"I could use the Analytical Engine to tell stories? Not just any stories, but stories told with numbers!\"\n",
      "\n",
      "The idea captivated her. She envisioned numbers dancing across the machine's gears, weaving tales of brave knights, talking animals, and faraway lands. Each number would represent a character, an action, or a feeling, creating a symphony of storytelling unlike anything the world had ever seen.\n",
      "\n",
      "With newfound determination, Ada set to work. She spent hours studying the workings of the Analytical Engine, learning its language of gears and levers. She poured over books on mathematics, logic, and storytelling, searching for ways to translate words into the language of numbers.\n",
      "\n",
      "Days turned into weeks, and weeks into months. Ada's room became a workshop filled with scribbled notes, diagrams, and miniature models of the Analytical Engine. She experimented with different combinations of numbers, testing their ability to convey emotions, actions, and even the passage of time.\n",
      "\n",
      "One evening, as the sun began to set, Ada finally achieved a breakthrough. She had devised a system of using numbers to represent characters, actions, and dialogue. With a triumphant smile, she sat down at the Analytical Engine and began to input her first story.\n",
      "\n",
      "The machine whirred to life, its gears clicking and clacking as it processed Ada's instructions. Slowly, but surely, the story began to unfold. Numbers danced across the machine's display, transforming into a captivating tale of a brave princess who embarked on a quest to save her kingdom.\n",
      "\n",
      "As Ada watched the story unfold, she felt a sense of accomplishment wash over her. She had not only created a new way of telling stories, but she had also unlocked the hidden potential of numbers. They were no longer just symbols on a page; they were the key to a world of endless possibilities.\n",
      "\n",
      "From that day forward, Ada continued to explore the world of numbers and storytelling. She wrote countless stories, each one more intricate and imaginative than the last. She became known throughout the land as the \"Enchantress of Numbers,\" a title she wore with pride.\n",
      "\n",
      "And so, the legacy of Ada Lovelace, the world's first computer programmer, was born. Her pioneering work not only paved the way for the modern computer but also opened the door to a new era of storytelling, where numbers could dance and sing, weaving tales of wonder and imagination for generations to come.\n",
      "An error occurred: Cannot get the Candidate text.\n",
      "Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\n",
      "Content:\n",
      "{}\n",
      "Candidate:\n",
      "{\n",
      "  \"finish_reason\": \"RECITATION\",\n",
      "  \"safety_ratings\": [\n",
      "    {\n",
      "      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "      \"probability\": \"NEGLIGIBLE\",\n",
      "      \"probability_score\": 0.061767578,\n",
      "      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n",
      "      \"severity_score\": 0.09667969\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "      \"probability\": \"NEGLIGIBLE\",\n",
      "      \"probability_score\": 0.041992188,\n",
      "      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n",
      "      \"severity_score\": 0.026000977\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "      \"probability\": \"NEGLIGIBLE\",\n",
      "      \"probability_score\": 0.061035156,\n",
      "      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n",
      "      \"severity_score\": 0.087402344\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "      \"probability\": \"NEGLIGIBLE\",\n",
      "      \"probability_score\": 0.14941406,\n",
      "      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n",
      "      \"severity_score\": 0.18359375\n",
      "    }\n",
      "  ],\n",
      "  \"citation_metadata\": {\n",
      "    \"citations\": [\n",
      "      {\n",
      "        \"start_index\": 1458,\n",
      "        \"end_index\": 1579,\n",
      "        \"uri\": \"https://samschicken.wordpress.com/monday/magazine-editorial-design/golden-rule/\"\n",
      "      },\n",
      "      {\n",
      "        \"start_index\": 1492,\n",
      "        \"end_index\": 1623,\n",
      "        \"uri\": \"https://www.numerade.com/ask/question/is-the-series-4-103036108-114-a-fibonacci-type-sequence-if-it-isgive-the-next-two-terms-in-the-sequence-if-it-is-not-state-that-the-sequence-is-not-a-fibonacci-type-sequence-0-188-200-120180-11424/\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Let's try again with a new story idea.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mchat_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mchat_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_loop\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m         user_age \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter the age of the child (e.g., 5): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m         input_from_user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat would you like the story to be about? \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m         summary_object \u001b[38;5;241m=\u001b[39m generate_summary(input_from_user, user_age)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def chat_loop():\n",
    "    while True:\n",
    "        user_age = int(input(\"Enter the age of the child (e.g., 5): \"))\n",
    "        input_from_user = input(\"What would you like the story to be about? \")\n",
    "        \n",
    "        summary_object = generate_summary(input_from_user, user_age)\n",
    "        user_response = input(\"Do you want to continue with this summary? (YES/NO): \")\n",
    "        \n",
    "        while user_response.upper() == \"NO\":\n",
    "            question = input(\"Do you want a new story or make changes to this storyline? (NEW STORY/MODIFY): \")\n",
    "            if question.upper() == \"MODIFY\":          \n",
    "                new_prompt = input(\"Please provide the changes you want to see in this story: \")\n",
    "                summary_prompt = f\"Make the following changes to your story summary, ensuring you maintain complete summaries for all three chapters: {new_prompt}\"\n",
    "                summary_object = summary_chain.run(input_prompt=summary_prompt, user_age=user_age)\n",
    "                print(\"\\nUpdated Complete Story Summary:\")\n",
    "                print(summary_object)\n",
    "            elif question.upper() == \"NEW STORY\":\n",
    "                input_from_user = input(\"Let's build you a new story! What would you like to hear a story about? \")\n",
    "                summary_object = generate_summary(input_from_user, user_age)\n",
    "            else:\n",
    "                print(\"Invalid option. Please choose NEW STORY or MODIFY.\")\n",
    "                continue\n",
    "            \n",
    "            user_response = input(\"Do you want to continue with this summary? (YES/NO): \")\n",
    "\n",
    "        if user_response.upper() == \"YES\":\n",
    "            try:\n",
    "                generated_chapters = generate_story_in_chapters(summary_object, user_age)\n",
    "                print(\"\\nFull story generated successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                print(\"Let's try again with a new story idea.\")\n",
    "                continue\n",
    "        \n",
    "        continue_response = input(\"Do you want to create another story? (YES/NO): \")\n",
    "        if continue_response.upper() != \"YES\":\n",
    "            print(\"Thank you for using the story generator. Goodbye!\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515ce15e-ac72-40c0-909f-9f71e7b79e24",
   "metadata": {},
   "source": [
    "## ***To Check For:***\n",
    "- If characters extraction function is working\n",
    "- If chapter 3 has the consolidated summary of chapter 1 & chapter 2\n",
    "\n",
    "## ***To Add:***\n",
    "- Ouput Parsing function from LangChain to ensure output format is consistent every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed3dd3-9840-412b-b84a-ab1793e102b7",
   "metadata": {},
   "source": [
    "#  ***Adding Extended input***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93c6e8e8-69b7-4e87-89c4-77efad0aad1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the age of the child (e.g., 5):  6\n",
      "What would you like the story to be about?  A story about 4 friends and a dog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Story Summary:\n",
      "## Story Introduction:\n",
      "\n",
      "Four best friends, Maya, Leo, Ben, and Chloe, lived on a quiet street filled with laughter and sunshine. They were inseparable, always embarking on exciting adventures together. One sunny afternoon, while playing in Maya's backyard, they stumbled upon a curious sight â€“ a small, fluffy dog hiding under the bushes.\n",
      "\n",
      "## Chapter 1: A Mysterious New Friend\n",
      "\n",
      "The friends cautiously approached the dog, who whimpered and trembled. Maya, known for her gentle nature, knelt down and offered a piece of her sandwich. The dog, with hesitant steps, approached and licked her hand. They named him Lucky, for he seemed to have found his lucky day.\n",
      "\n",
      "## Chapter 2: The Great Treasure Hunt\n",
      "\n",
      "Lucky, full of energy and playful spirit, quickly became part of the gang. One day, while exploring the nearby woods, they discovered an old, dusty map. It depicted a hidden treasure buried somewhere within the woods. Armed with shovels and boundless enthusiasm, the friends, led by Lucky's keen sense of smell, embarked on a thrilling treasure hunt.\n",
      "\n",
      "## Chapter 3: Friendship and Buried Treasure\n",
      "\n",
      "After hours of searching, following clues and digging through leaves, they finally unearthed a wooden chest. Inside, they found not gold or jewels, but a collection of old toys and games. Disappointed at first, they soon realized the true treasure was the adventure they shared and the unbreakable bond of their friendship. They spent the rest of the day playing with the newfound treasures, Lucky by their side, barking with joy. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to continue with this summary? (YES/NO):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chapter 1 Generated:\n",
      "## Chapter 1: A Mysterious New Friend\n",
      "\n",
      "The sun peeked over the horizon, painting the sky with vibrant hues of orange and pink. The air was crisp and cool, carrying the sweet scent of wildflowers. Maya, Leo, and Ben, three inseparable friends, skipped along the dusty path, their backpacks bouncing merrily on their shoulders. They were on their way to their favorite spot by the river, a hidden haven where they could spend hours building sandcastles, chasing butterflies, and sharing stories.\n",
      "\n",
      "As they approached the riverbank, a faint whimpering caught their attention. They stopped, their eyes scanning the area. There, huddled beneath a willow tree, was a small, brown dog. His fur was matted and dirty, and his tail tucked between his legs. He looked scared and alone.\n",
      "\n",
      "\"Oh, poor thing,\" Maya whispered, her heart melting with compassion. \"He must be lost.\"\n",
      "\n",
      "Leo, the adventurous one, took a step forward. \"Let's go see him,\" he said, his voice filled with curiosity.\n",
      "\n",
      "Ben, the cautious one, hesitated. \"He might bite,\" he mumbled, his eyes wide with apprehension.\n",
      "\n",
      "Maya, ever the peacemaker, reassured him. \"Don't worry, Ben. We'll be careful.\"\n",
      "\n",
      "Together, they approached the dog slowly, their hands outstretched in a gesture of friendship. The dog cowered, whimpering louder. Maya knelt down, her voice soft and gentle.\n",
      "\n",
      "\"It's okay, little one,\" she said, offering him a piece of her sandwich. \"We're not going to hurt you.\"\n",
      "\n",
      "The dog hesitated for a moment, then cautiously took a step forward. He sniffed the sandwich, his tail twitching nervously. Then, with a lick of his tongue, he devoured it in one gulp.\n",
      "\n",
      "Maya smiled. \"See, he's just hungry,\" she said.\n",
      "\n",
      "Leo reached into his backpack and pulled out a water bottle. He filled the cap and offered it to the dog. The dog lapped up the water gratefully, his thirst quenched.\n",
      "\n",
      "\"He's so thirsty,\" Ben said, his fear replaced with concern.\n",
      "\n",
      "The friends sat down beside the dog, gently stroking his fur. He seemed to relax in their presence, his whimpers fading into soft sighs.\n",
      "\n",
      "\"What should we name him?\" Maya asked.\n",
      "\n",
      "\"Lucky,\" Leo suggested. \"He's lucky we found him.\"\n",
      "\n",
      "Ben nodded in agreement. \"Lucky it is.\"\n",
      "\n",
      "From that day forward, Lucky became their constant companion. He followed them everywhere, his tail wagging with joy. They played fetch with him, built him a cozy bed in their treehouse, and shared their snacks with him. Lucky, in return, showered them with unconditional love and loyalty.\n",
      "\n",
      "One sunny afternoon, as they were playing by the river, Maya noticed a small, metal tag attached to Lucky's collar. It was engraved with a name and a phone number.\n",
      "\n",
      "\"Look!\" she exclaimed, pointing to the tag. \"He has an owner!\"\n",
      "\n",
      "Excitedly, they called the number. A woman answered, her voice filled with relief. She explained that Lucky had gone missing a few days ago and that she had been searching for him ever since.\n",
      "\n",
      "The friends were heartbroken to hear that Lucky had a home to return to, but they knew it was the right thing to do. They arranged to meet the woman at the park the next day.\n",
      "\n",
      "When they arrived at the park, the woman was waiting for them, tears of joy streaming down her face. She embraced Lucky tightly, thanking the friends for taking such good care of him.\n",
      "\n",
      "\"He's my best friend,\" she said, her voice choked with emotion. \"I don't know what I would have done without him.\"\n",
      "\n",
      "The friends watched as Lucky and his owner walked away, their hearts filled with a bittersweet mix of sadness and joy. They were sad to say goodbye to their furry friend, but they were happy that he was back where he belonged.\n",
      "\n",
      "As they walked home, they couldn't help but smile. They had made a new friend, even if it was only for a short time. And they knew that Lucky would always hold a special place in their hearts.\n",
      "\n",
      "Chapter 2 Generated:\n",
      "## Chapter 2: The Great Treasure Hunt\n",
      "\n",
      "The sun peeked through the leaves, casting playful shadows on the forest floor. Maya, Leo, Ben, and Lucky, their newfound furry friend, skipped along the winding path, their laughter echoing through the trees. Lucky, his tail wagging furiously, led the way, his nose twitching excitedly as he sniffed the air.\n",
      "\n",
      "\"I bet the treasure is buried near the old oak tree,\" Leo declared, pointing to a giant oak with gnarled branches that stretched towards the sky.\n",
      "\n",
      "\"Or maybe it's hidden by the babbling brook,\" Maya suggested, her eyes sparkling with anticipation.\n",
      "\n",
      "Ben, ever the cautious one, chimed in, \"We should check the abandoned cabin first. Pirates love hiding their loot in spooky places.\"\n",
      "\n",
      "With Lucky bounding ahead, they explored every nook and cranny of the woods. They dug beneath the oak tree, its roots tickling their fingers. They searched the babbling brook, its cool water swirling around their bare feet. They even ventured into the abandoned cabin, its dusty floorboards creaking under their weight.\n",
      "\n",
      "But the treasure remained elusive. Disappointment began to cloud their faces.\n",
      "\n",
      "\"Maybe the map is just a trick,\" Ben mumbled, kicking a fallen leaf in frustration.\n",
      "\n",
      "\"Don't give up!\" Maya exclaimed, her chin held high. \"We just need to keep looking.\"\n",
      "\n",
      "Suddenly, Lucky stopped in his tracks, his nose pointed towards a large, flat rock hidden beneath a tangle of vines. He barked excitedly and began digging with his paws.\n",
      "\n",
      "Intrigued, the friends rushed over and joined Lucky in his excavation. They pulled away the vines, revealing a smooth, moss-covered rock. With trembling hands, they lifted the rock, their hearts pounding with anticipation.\n",
      "\n",
      "Beneath the rock lay a wooden chest, its hinges rusted and its surface covered in cobwebs. With bated breath, they opened the chest, revealing a dazzling array of treasures.\n",
      "\n",
      "There were sparkling gold coins, shimmering jewels, and even a small, leather-bound book filled with pirate stories. The friends gasped in awe, their eyes wide with wonder.\n",
      "\n",
      "\"We found it!\" Leo shouted, jumping for joy.\n",
      "\n",
      "\"We're rich!\" Ben exclaimed, his eyes gleaming with excitement.\n",
      "\n",
      "Maya, ever the practical one, reminded them, \"We should share the treasure with everyone in the village. They could use the gold to build a new school or a playground.\"\n",
      "\n",
      "The friends agreed, their hearts filled with generosity and the thrill of their grand adventure. They returned to the village, their pockets overflowing with gold and their hearts brimming with joy. They shared their treasure with everyone, and the village celebrated their bravery and kindness.\n",
      "\n",
      "From that day forward, Maya, Leo, Ben, and Lucky became known as the \"Treasure Hunters of the Whispering Woods,\" their names forever etched in the village's history. And Lucky, the once lost and lonely dog, found a loving home and a place he truly belonged.\n",
      "\n",
      "Chapter 3 Generated:\n",
      "The sun peeked through the leaves, casting playful shadows on the forest floor. Maya, Leo, and Ben, along with their furry companion Lucky, were on a mission. They clutched a worn-out map, its edges frayed and its lines faded with time. It was a treasure map, passed down from Maya's grandfather, and it promised an adventure beyond their wildest dreams.\n",
      "\n",
      "For hours, they followed the map's cryptic clues. They crossed a babbling brook, its water sparkling like diamonds in the sunlight. They climbed over moss-covered rocks, their laughter echoing through the silent woods. They even ventured into an abandoned cabin, its windows boarded up and its roof caved in.\n",
      "\n",
      "Finally, after hours of searching, they reached their destination. A large X marked the spot on the map, and beneath it lay a pile of fallen leaves. With eager hands, they began to dig, their excitement growing with each shovelful of earth.\n",
      "\n",
      "Suddenly, their shovels struck something hard. It was a wooden chest, its surface weathered and its hinges rusted. With bated breath, they lifted the lid, revealing the contents within.\n",
      "\n",
      "But instead of gold and jewels, as they had imagined, the chest held a collection of old toys and games. There were wooden cars, dusty dolls, and a tattered board game. Disappointment washed over them like a cold wave.\n",
      "\n",
      "\"This isn't a treasure,\" Leo muttered, his voice heavy with sadness.\n",
      "\n",
      "\"We spent all day searching for nothing,\" Ben added, his shoulders slumping.\n",
      "\n",
      "But Maya, ever the optimist, saw things differently. She picked up a wooden car, its paint chipped and its wheels worn. She remembered the countless hours she had spent playing with similar toys as a child, her imagination soaring as she raced them across the living room floor.\n",
      "\n",
      "\"This isn't just a toy,\" she said, her voice filled with wonder. \"It's a reminder of all the adventures we've had together. It's a symbol of our friendship.\"\n",
      "\n",
      "Leo and Ben looked at each other, their disappointment fading away. They realized that Maya was right. The true treasure wasn't the gold or jewels they had hoped for, but the memories they had made along the way.\n",
      "\n",
      "They spent the rest of the day playing with the newfound treasures. They raced the wooden cars down a makeshift track, their laughter filling the air. They dressed up the dolls in leaves and twigs, creating their own little world. And they played the board game, even though some of the pieces were missing.\n",
      "\n",
      "As the sun began to set, casting long shadows across the forest, they knew that this was an adventure they would never forget. They had found a treasure more valuable than gold, a treasure that would last a lifetime â€“ the unbreakable bond of their friendship.\n",
      "\n",
      "Lucky, their loyal companion, barked with joy, his tail wagging furiously. He had been by their side throughout their adventure, sharing their laughter and their tears. He was a part of their friendship, and he would always be there for them, no matter what.\n",
      "\n",
      "As they walked back home, hand in hand, they knew that their friendship was the greatest treasure they could ever find. It was a treasure that would guide them through life's adventures, big and small. And they knew that no matter what challenges they faced, they would always have each other to rely on.\n",
      "Total time taken: 54.35349082946777 seconds\n",
      "\n",
      "Full story generated successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to create another story? (YES/NO):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using the story generator. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI\n",
    "vertexai.init(project='adsp-capstone-once-upon', location = 'us-central1')\n",
    "# Initialize the LLM\n",
    "llm = VertexAI(model_name=\"gemini-1.0-pro\", max_output_tokens=8192, max_tokens = 32000)\n",
    "# Initialize the text summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Define prompt template for the summary\n",
    "summary_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input_prompt\", \"user_age\"],\n",
    "    template=\"\"\"\n",
    "    Generate a complete summary for a 3-chapter story that is engaging for a {user_age}-year-old child. The story should be about: {input_prompt}\n",
    "    Your response must include:\n",
    "    1. A brief introduction to the story\n",
    "    2. Chapter 1 summary (at least 100 words)\n",
    "    3. Chapter 2 summary (at least 100 words)\n",
    "    4. Chapter 3 summary (at least 100 words)\n",
    "    Format your response as follows:\n",
    "    Story Introduction: [Your introduction here]\n",
    "    Chapter 1: [Title]\n",
    "    [Chapter 1 summary]\n",
    "    Chapter 2: [Title]\n",
    "    [Chapter 2 summary]\n",
    "    Chapter 3: [Title]\n",
    "    [Chapter 3 summary]\n",
    "    Ensure each chapter summary is complete and shows a clear progression of the story.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create LLMChain for summary generation\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template)\n",
    "\n",
    "def generate_summary(input_prompt, user_age):\n",
    "    while True:\n",
    "        summary = summary_chain.run(input_prompt=input_prompt, user_age=user_age)\n",
    "        print(\"Complete Story Summary:\")\n",
    "        print(summary)\n",
    "        return summary\n",
    "\n",
    "# Define updated prompt template for chapter generation\n",
    "chapter_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"chapter_title\", \"previous_summary\", \"chapter_summary\", \"characters\", \"chapter_number\", \"user_age\"],\n",
    "    template=(\n",
    "        \"This is the outline for the chapter: Title: {chapter_title}.\\n\"\n",
    "        \"Summary of previous chapters: {previous_summary}.\\n\"\n",
    "        \"Summary for this chapter: {chapter_summary}.\\n\"\n",
    "        \"Characters and their associated personalities and behaviors so far: {characters}.\\n\"\n",
    "        \"Based on this outline, generate a LENGTHY chapter {chapter_number} of AT LEAST 1000 WORD TOKENS. \"\n",
    "        \"The story should be written in PARAGRAPHS (not bullet points) and fit for a {user_age}-year-old kid in English. \"\n",
    "        \"Please ensure the chapter has AT LEAST 1000 words and does not include any additional notes. \"\n",
    "        \"Overall, the total number of tokens should be less than 8000 tokens (all characters included).\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create LLMChain for chapter generation\n",
    "chapter_chain = LLMChain(llm=llm, prompt=chapter_prompt_template)\n",
    "\n",
    "def generate_chapter(chapter_title, previous_summary, chapter_summary, characters, chapter_number, user_age):\n",
    "    chapter = chapter_chain.run(\n",
    "        chapter_title=chapter_title,\n",
    "        previous_summary=previous_summary,\n",
    "        chapter_summary=chapter_summary,\n",
    "        characters=characters,\n",
    "        chapter_number=chapter_number,\n",
    "        user_age=user_age\n",
    "    )\n",
    "    print(f\"\\nChapter {chapter_number} Generated:\")\n",
    "    print(chapter)\n",
    "    return chapter\n",
    "\n",
    "# Add this new prompt template for character extraction\n",
    "character_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"story_text\"],\n",
    "    template=\"\"\"\n",
    "    Extract the main characters from the following story text, along with a brief description of their personalities and behaviors:\n",
    "\n",
    "    {story_text}\n",
    "\n",
    "    Format your response as a list of characters, each with a short description:\n",
    "    - Character Name: Brief description of personality and behavior\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create LLMChain for character extraction\n",
    "character_extraction_chain = LLMChain(llm=llm, prompt=character_extraction_prompt)\n",
    "\n",
    "# Add character extraction function\n",
    "# Update the extract_characters function\n",
    "def extract_characters(input_story):\n",
    "    characters = character_extraction_chain.run(story_text=input_story)\n",
    "    return characters\n",
    "\n",
    "def generate_story_in_chapters(summary_object, user_age):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    chapters = re.split(r'Chapter \\d+:', summary_object)[1:]\n",
    "    if len(chapters) != 3:\n",
    "        raise ValueError(\"The summary does not contain exactly 3 chapters.\")\n",
    "    \n",
    "    chapter_structures = [f\"Chapter {i+1}:{chapter}\" for i, chapter in enumerate(chapters)]\n",
    "    \n",
    "    # Generate Chapter 1\n",
    "    chapter1 = generate_chapter(chapter_structures[0].split(':')[1].strip(), \"\", chapters[0], \"\", 1, user_age)\n",
    "    characters = extract_characters(chapter1)\n",
    "    chapter1_summary = summarizer(chapter1, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Generate Chapter 2\n",
    "    chapter2 = generate_chapter(chapter_structures[1].split(':')[1].strip(), chapter1_summary, chapters[1], characters, 2, user_age)\n",
    "    chapter2_summary = summarizer(chapter2, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Generate Chapter 3\n",
    "    previous_summary = f\"Chapter 1: {chapter1_summary}\\nChapter 2: {chapter2_summary}\"\n",
    "    chapter3 = generate_chapter(chapter_structures[2].split(':')[1].strip(), previous_summary, chapters[2], characters, 3, user_age)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time taken: {total_time} seconds\")\n",
    "    \n",
    "    return chapter1, chapter2, chapter3\n",
    "\n",
    "def chat_loop():\n",
    "    while True:\n",
    "        user_age = int(input(\"Enter the age of the child (e.g., 5): \"))\n",
    "        input_from_user = input(\"What would you like the story to be about? \")\n",
    "        \n",
    "        summary_object = generate_summary(input_from_user, user_age)\n",
    "        user_response = input(\"Do you want to continue with this summary? (YES/NO): \")\n",
    "        \n",
    "        while user_response.upper() == \"NO\":\n",
    "            question = input(\"Do you want a new story or make changes to this storyline? (NEW STORY/MODIFY): \")\n",
    "            if question.upper() == \"MODIFY\":          \n",
    "                new_prompt = input(\"Please provide the changes you want to see in this story: \")\n",
    "                summary_prompt = f\"Make the following changes to your story summary, ensuring you maintain complete summaries for all three chapters: {new_prompt}\"\n",
    "                summary_object = summary_chain.run(input_prompt=summary_prompt, user_age=user_age)\n",
    "                print(\"\\nUpdated Complete Story Summary:\")\n",
    "                print(summary_object)\n",
    "            elif question.upper() == \"NEW STORY\":\n",
    "                input_from_user = input(\"Let's build you a new story! What would you like to hear a story about? \")\n",
    "                summary_object = generate_summary(input_from_user, user_age)\n",
    "            else:\n",
    "                print(\"Invalid option. Please choose NEW STORY or MODIFY.\")\n",
    "                continue\n",
    "            \n",
    "            user_response = input(\"Do you want to continue with this summary? (YES/NO): \")\n",
    "        \n",
    "        if user_response.upper() == \"YES\":\n",
    "            try:\n",
    "                generated_chapters = generate_story_in_chapters(summary_object, user_age)\n",
    "                print(\"\\nFull story generated successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                print(\"Let's try again with a new story idea.\")\n",
    "                continue\n",
    "        \n",
    "        continue_response = input(\"Do you want to create another story? (YES/NO): \")\n",
    "        if continue_response.upper() != \"YES\":\n",
    "            print(\"Thank you for using the story generator. Goodbye!\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c459ca-9db9-4a2a-bbb2-738004baa2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mVertexAI\u001b[0m\n",
      "Params: {'model_name': 'gemini-1.0-pro', 'temperature': 0.0, 'max_output_tokens': 8192, 'candidate_count': 1, 'top_k': 40, 'top_p': 0.95}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a809d2-cc4f-4841-b349-c8061e861069",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.16\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39326abd-c0e9-4e8e-ba1c-bb83f5e97742",
   "metadata": {},
   "source": [
    "### ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e0245-133c-4a90-852f-39b00803f965",
   "metadata": {},
   "source": [
    "## ***Adding Output Parser for Structure and Consistency***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3074af-4de7-4bde-a85c-cfe5bfaa2ded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (/opt/conda/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate, HumanMessagePromptTemplate\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#from langchain.llms import VertexAI\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_vertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VertexAI\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_google_vertexai/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     FunctionCallingConfig,\n\u001b[1;32m      3\u001b[0m     FunctionDeclaration,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     Type,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_vertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     HarmBlockThreshold,\n\u001b[1;32m     11\u001b[0m     HarmCategory,\n\u001b[1;32m     12\u001b[0m     SafetySetting,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_vertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_structured_runnable\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_vertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatVertexAI\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_google_vertexai/_enums.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerative_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     HarmBlockThreshold,\n\u001b[1;32m      3\u001b[0m     HarmCategory,\n\u001b[1;32m      4\u001b[0m     SafetySetting,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHarmBlockThreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHarmCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSafetySetting\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/__init__.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"The vertexai module.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m aiplatform_version\n\u001b[1;32m     21\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m aiplatform_version\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/__init__.py:41\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hyperparameter_tuning\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeaturestore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     EntityType,\n\u001b[1;32m     38\u001b[0m     Feature,\n\u001b[1;32m     39\u001b[0m     Featurestore,\n\u001b[1;32m     40\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     42\u001b[0m     MatchingEngineIndex,\n\u001b[1;32m     43\u001b[0m     MatchingEngineIndexEndpoint,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m uploader_tracker\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/matching_engine/__init__.py:26\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     MatchingEngineIndex,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine_index_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     BruteForceConfig \u001b[38;5;28;01mas\u001b[39;00m MatchingEngineBruteForceAlgorithmConfig,\n\u001b[1;32m     23\u001b[0m     MatchingEngineIndexConfig \u001b[38;5;28;01mas\u001b[39;00m MatchingEngineIndexConfig,\n\u001b[1;32m     24\u001b[0m     TreeAhConfig \u001b[38;5;28;01mas\u001b[39;00m MatchingEngineTreeAhAlgorithmConfig,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine_index_endpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     MatchingEngineIndexEndpoint,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatchingEngineIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatchingEngineIndexEndpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatchingEngineTreeAhAlgorithmConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/matching_engine/matching_engine_index_endpoint.py:34\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     machine_resources \u001b[38;5;28;01mas\u001b[39;00m gca_machine_resources_compat,\n\u001b[1;32m     28\u001b[0m     matching_engine_index_endpoint \u001b[38;5;28;01mas\u001b[39;00m gca_matching_engine_index_endpoint,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     encryption_spec \u001b[38;5;28;01mas\u001b[39;00m gca_encryption_spec,\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_protos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m match_service_pb2\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_protos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     36\u001b[0m     match_service_pb2_grpc,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m field_mask_pb2\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/matching_engine/_protos/match_service_pb2.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2022 Google LLC\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# source: google/cloud/aiplatform/matching_engine/_protos/match_service.proto\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m status_pb2 \u001b[38;5;28;01mas\u001b[39;00m google_dot_rpc_dot_status__pb2\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (/opt/conda/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "#from langchain.llms import VertexAI\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.chains import LLMChain\n",
    "from transformers import pipeline\n",
    "import vertexai\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project='adsp-capstone-once-upon', location='us-central1')\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = VertexAI(model_name=\"gemini-1.0-pro\", max_output_tokens=8192, max_tokens=32000)\n",
    "\n",
    "# Initialize the text summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Define response schemas for structured output\n",
    "summary_response_schemas = [\n",
    "    ResponseSchema(name=\"introduction\", description=\"A brief introduction to the story\"),\n",
    "    ResponseSchema(name=\"chapter_1\", description=\"Summary of Chapter 1 (at least 100 words)\"),\n",
    "    ResponseSchema(name=\"chapter_2\", description=\"Summary of Chapter 2 (at least 100 words)\"),\n",
    "    ResponseSchema(name=\"chapter_3\", description=\"Summary of Chapter 3 (at least 100 words)\"),\n",
    "]\n",
    "\n",
    "summary_output_parser = StructuredOutputParser.from_response_schemas(summary_response_schemas)\n",
    "\n",
    "# Update the summary prompt template\n",
    "summary_prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Generate a complete summary for a 3-chapter story that is engaging for a {user_age}-year-old child. The story should be about: {input_prompt}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Ensure each chapter summary is complete and shows a clear progression of the story.\"\"\"\n",
    "        )\n",
    "    ],\n",
    "    input_variables=[\"input_prompt\", \"user_age\"],\n",
    "    partial_variables={\"format_instructions\": summary_output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Create LLMChain for summary generation\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template)\n",
    "\n",
    "def generate_summary(input_prompt, user_age):\n",
    "    summary = summary_chain.run(input_prompt=input_prompt, user_age=user_age)\n",
    "    parsed_summary = summary_output_parser.parse(summary)\n",
    "    print(\"Complete Story Summary:\")\n",
    "    print(f\"Introduction: {parsed_summary['introduction']}\")\n",
    "    print(f\"Chapter 1: {parsed_summary['chapter_1']}\")\n",
    "    print(f\"Chapter 2: {parsed_summary['chapter_2']}\")\n",
    "    print(f\"Chapter 3: {parsed_summary['chapter_3']}\")\n",
    "    return parsed_summary\n",
    "\n",
    "# Define response schemas for chapter generation\n",
    "chapter_response_schemas = [\n",
    "    ResponseSchema(name=\"content\", description=\"The full content of the chapter\"),\n",
    "    ResponseSchema(name=\"word_count\", description=\"The word count of the chapter\"),\n",
    "]\n",
    "\n",
    "chapter_output_parser = StructuredOutputParser.from_response_schemas(chapter_response_schemas)\n",
    "\n",
    "# Update the chapter prompt template\n",
    "chapter_prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"This is the outline for the chapter: Title: {chapter_title}.\n",
    "Summary of previous chapters: {previous_summary}.\n",
    "Summary for this chapter: {chapter_summary}.\n",
    "Characters and their associated personalities and behaviors so far: {characters}.\n",
    "\n",
    "Based on this outline, generate a LENGTHY chapter {chapter_number} of AT LEAST 1000 WORD TOKENS.\n",
    "The story should be written in PARAGRAPHS (not bullet points) and fit for a {user_age}-year-old kid in English.\n",
    "Please ensure the chapter has AT LEAST 1000 words and does not include any additional notes.\n",
    "Overall, the total number of tokens should be less than 8000 tokens (all characters included).\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "        )\n",
    "    ],\n",
    "    input_variables=[\"chapter_title\", \"previous_summary\", \"chapter_summary\", \"characters\", \"chapter_number\", \"user_age\"],\n",
    "    partial_variables={\"format_instructions\": chapter_output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Create LLMChain for chapter generation\n",
    "chapter_chain = LLMChain(llm=llm, prompt=chapter_prompt_template)\n",
    "\n",
    "def generate_chapter(chapter_title, previous_summary, chapter_summary, characters, chapter_number, user_age):\n",
    "    chapter = chapter_chain.run(\n",
    "        chapter_title=chapter_title,\n",
    "        previous_summary=previous_summary,\n",
    "        chapter_summary=chapter_summary,\n",
    "        characters=characters,\n",
    "        chapter_number=chapter_number,\n",
    "        user_age=user_age\n",
    "    )\n",
    "    parsed_chapter = chapter_output_parser.parse(chapter)\n",
    "    print(f\"\\nChapter {chapter_number} Generated:\")\n",
    "    print(f\"Word count: {parsed_chapter['word_count']}\")\n",
    "    print(parsed_chapter['content'])\n",
    "    return parsed_chapter\n",
    "\n",
    "# Update the character extraction prompt and parser\n",
    "character_response_schemas = [\n",
    "    ResponseSchema(name=\"characters\", description=\"List of main characters with their descriptions\")\n",
    "]\n",
    "\n",
    "character_output_parser = StructuredOutputParser.from_response_schemas(character_response_schemas)\n",
    "\n",
    "character_extraction_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Extract the main characters from the following story text, along with a brief description of their personalities and behaviors:\n",
    "\n",
    "{story_text}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Format your response as a list of characters, each with a short description:\n",
    "- Character Name: Brief description of personality and behavior\"\"\"\n",
    "        )\n",
    "    ],\n",
    "    input_variables=[\"story_text\"],\n",
    "    partial_variables={\"format_instructions\": character_output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Create LLMChain for character extraction\n",
    "character_extraction_chain = LLMChain(llm=llm, prompt=character_extraction_prompt)\n",
    "\n",
    "# Update the extract_characters function\n",
    "def extract_characters(input_story):\n",
    "    characters = character_extraction_chain.run(story_text=input_story)\n",
    "    parsed_characters = character_output_parser.parse(characters)\n",
    "    return parsed_characters['characters']\n",
    "\n",
    "def generate_story_in_chapters(summary_object, user_age):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate Chapter 1\n",
    "    chapter1 = generate_chapter(\"Chapter 1\", \"\", summary_object['chapter_1'], \"\", 1, user_age)\n",
    "    characters = extract_characters(chapter1['content'])\n",
    "    chapter1_summary = summarizer(chapter1['content'], max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Generate Chapter 2\n",
    "    chapter2 = generate_chapter(\"Chapter 2\", chapter1_summary, summary_object['chapter_2'], characters, 2, user_age)\n",
    "    chapter2_summary = summarizer(chapter2['content'], max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Generate Chapter 3\n",
    "    previous_summary = f\"Chapter 1: {chapter1_summary}\\nChapter 2: {chapter2_summary}\"\n",
    "    chapter3 = generate_chapter(\"Chapter 3\", previous_summary, summary_object['chapter_3'], characters, 3, user_age)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time taken: {total_time} seconds\")\n",
    "    \n",
    "    return chapter1, chapter2, chapter3\n",
    "\n",
    "def chat_loop():\n",
    "    while True:\n",
    "        user_age = int(input(\"Enter the age of the child (e.g., 5): \"))\n",
    "        input_from_user = input(\"What would you like the story to be about? \")\n",
    "        \n",
    "        summary_object = generate_summary(input_from_user, user_age)\n",
    "        user_response = input(\"Do you want to continue with this summary? (YES/NO): \")\n",
    "        \n",
    "        while user_response.upper() == \"NO\":\n",
    "            question = input(\"Do you want a new story or make changes to this storyline? (NEW STORY/MODIFY): \")\n",
    "            if question.upper() == \"MODIFY\":          \n",
    "                new_prompt = input(\"Please provide the changes you want to see in this story: \")\n",
    "                summary_prompt = f\"Make the following changes to your story summary, ensuring you maintain complete summaries for all three chapters: {new_prompt}\"\n",
    "                summary_object = generate_summary(summary_prompt, user_age)\n",
    "            elif question.upper() == \"NEW STORY\":\n",
    "                input_from_user = input(\"Let's build you a new story! What would you like to hear a story about? \")\n",
    "                summary_object = generate_summary(input_from_user, user_age)\n",
    "            else:\n",
    "                print(\"Invalid option. Please choose NEW STORY or MODIFY.\")\n",
    "                continue\n",
    "            \n",
    "            user_response = input(\"Do you want to continue with this summary? (YES/NO): \")\n",
    "        \n",
    "        if user_response.upper() == \"YES\":\n",
    "            try:\n",
    "                generated_chapters = generate_story_in_chapters(summary_object, user_age)\n",
    "                print(\"\\nFull story generated successfully!\")\n",
    "                # Here you can add code to save or further process the generated chapters\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                print(\"Let's try again with a new story idea.\")\n",
    "                continue\n",
    "        \n",
    "        continue_response = input(\"Do you want to create another story? (YES/NO): \")\n",
    "        if continue_response.upper() != \"YES\":\n",
    "            print(\"Thank you for using the story generator. Goodbye!\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6487d9aa-4812-46bf-b463-7a56290ea423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 20:02:50.779010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 20:02:52.327685: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-10-15 20:02:52.327878: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-10-15 20:02:52.327892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n",
      "protobuf version: 3.19.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import google.protobuf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"protobuf version:\", google.protobuf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff0ae86-7032-4fcb-b648-71c4c8cd0e65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5897ba68-8bbc-45c7-ad34-d02698de5b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0rc1\n",
      "Uninstalling tensorflow-2.18.0rc1:\n",
      "  Successfully uninstalled tensorflow-2.18.0rc1\n",
      "Found existing installation: transformers 4.41.0\n",
      "Uninstalling transformers-4.41.0:\n",
      "  Successfully uninstalled transformers-4.41.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629f0223-d39a-4ad3-848d-fa1090fe0ea1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow==2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting transformers==4.41.0\n",
      "  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.62.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (3.12.1)\n",
      "Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (23.2)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.0)\n",
      "  Using cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.16.0)\n",
      "Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.0)\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (0.29.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0) (0.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0) (4.66.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.42.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2024.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.28.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.6)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.0) (2024.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
      "Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorboard-data-server, protobuf, keras, transformers, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.6.0\n",
      "    Uninstalling keras-3.6.0:\n",
      "      Successfully uninstalled keras-3.6.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.21.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.70.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\n",
      "parler-tts 0.1 requires transformers<4.41.0,>=4.39.0, but you have transformers 4.41.0 which is incompatible.\n",
      "streamlit 1.33.0 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.0 transformers-4.41.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.11.0 transformers==4.41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f372ce-44c9-40c3-be2d-74b33ecfef32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 3.19.6\n",
      "Uninstalling protobuf-3.19.6:\n",
      "  Successfully uninstalled protobuf-3.19.6\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf==3.19.0\n",
      "  Downloading protobuf-3.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (806 bytes)\n",
      "Downloading protobuf-3.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.26.1\n",
      "    Uninstalling protobuf-5.26.1:\n",
      "      Successfully uninstalled protobuf-5.26.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-api-core 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.21.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.70.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-artifact-registry 1.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-dlp 3.16.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-monitoring 2.19.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-pubsub 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-spanner 3.44.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-vision 3.7.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "grpc-google-iam-v1 0.12.7 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\n",
      "streamlit 1.33.0 requires protobuf<5,>=3.20, but you have protobuf 3.19.0 which is incompatible.\n",
      "tensorboard-plugin-profile 2.15.1 requires protobuf<5.0.0dev,>=3.19.6, but you have protobuf 3.19.0 which is incompatible.\n",
      "tensorflow-hub 0.16.1 requires protobuf>=3.19.6, but you have protobuf 3.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall protobuf -y\n",
    "\n",
    "!pip install protobuf==3.19.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12b19ac1-2ba0-422c-9014-d398ddae304c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                                0.3.3\n",
      "langchain-community                      0.0.34\n",
      "langchain-core                           0.3.10\n",
      "langchain-google-vertexai                1.0.3\n",
      "langchain-text-splitters                 0.3.0\n",
      "pydantic                                 2.9.2\n",
      "pydantic_core                            2.23.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep -E \"pydantic|langchain|langchain-core\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63e1cf57-cd8c-4f31-9fc4-05e38195d20c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.10/site-packages (0.3.10)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.135)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (4.10.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1421c0b-d88b-4f50-baa5-92ea919c92d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/opt/conda/lib/python3.10/site-packages/tensorflow/python/../libtensorflow_cc.so.2: undefined symbol: _ZN4toco9TocoFlags5Impl_66_i_give_permission_to_break_this_code_default_qdq_conversion_mode_E",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensorflow\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/__init__.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     45\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:46\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# (b/143110113)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/pywrap_tfe.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tfe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: /opt/conda/lib/python3.10/site-packages/tensorflow/python/../libtensorflow_cc.so.2: undefined symbol: _ZN4toco9TocoFlags5Impl_66_i_give_permission_to_break_this_code_default_qdq_conversion_mode_E"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9999d97-b3ea-4b95-989f-d9cd7da4cca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import re\n",
      "import time\n",
      "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
      "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
      "#from langchain.llms import VertexAI\n",
      "from langchain_google_vertexai import VertexAI\n",
      "from langchain.chains import LLMChain\n",
      "from transformers import pipeline\n",
      "import vertexai\n",
      "\n",
      "# Initialize Vertex AI\n",
      "vertexai.init(project='adsp-capstone-once-upon', location='us-central1')\n",
      "\n",
      "# Initialize the LLM\n",
      "llm = VertexAI(model_name=\"gemini-1.0-pro\", max_output_tokens=8192, max_tokens=32000)\n",
      "\n",
      "# Initialize the text summarization pipeline\n",
      "summarizer = pipeline(\"summarization\")\n",
      "\n",
      "# Define response schemas for structured output\n",
      "summary_response_schemas = [\n",
      "    ResponseSchema(name=\"introduction\", description=\"A brief introduction to the story\"),\n",
      "    ResponseSchema(name=\"chapter_1\", description=\"Summary of Chapter 1 (at least 100 words)\"),\n",
      "    ResponseSchema(name=\"chapter_2\", description=\"Summary of Chapter 2 (at least 100 words)\"),\n",
      "    ResponseSchema(name=\"chapter_3\", description=\"Summary of Chapter 3 (at least 100 words)\"),\n",
      "]\n",
      "\n",
      "summary_output_parser = StructuredOutputParser.from_response_schemas(summary_response_schemas)\n",
      "\n",
      "# Update the summary prompt template\n",
      "summary_prompt_template = ChatPromptTemplate(\n",
      "    messages=[\n",
      "        HumanMessagePromptTemplate.from_template(\n",
      "            \"\"\"Generate a complete summary for a 3-chapter story that is engaging for a {user_age}-year-old child. The story should be about: {input_prompt}\n",
      "\n",
      "{format_instructions}\n",
      "\n",
      "Ensure each chapter summary is complete and shows a clear progression of the story.\"\"\"\n",
      "        )\n",
      "    ],\n",
      "    input_variables=[\"input_prompt\", \"user_age\"],\n",
      "    partial_variables={\"format_instructions\": summary_output_parser.get_format_instructions()}\n",
      ")\n",
      "\n",
      "# Create LLMChain for summary generation\n",
      "summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template)\n",
      "\n",
      "def generate_summary(input_prompt, user_age):\n",
      "    summary = summary_chain.run(input_prompt=input_prompt, user_age=user_age)\n",
      "    parsed_summary = summary_output_parser.parse(summary)\n",
      "    print(\"Complete Story Summary:\")\n",
      "    print(f\"Introduction: {parsed_summary['introduction']}\")\n",
      "    print(f\"Chapter 1: {parsed_summary['chapter_1']}\")\n",
      "    print(f\"Chapter 2: {parsed_summary['chapter_2']}\")\n",
      "    print(f\"Chapter 3: {parsed_summary['chapter_3']}\")\n",
      "    return parsed_summary\n",
      "\n",
      "# Define response schemas for chapter generation\n",
      "chapter_response_schemas = [\n",
      "    ResponseSchema(name=\"content\", description=\"The full content of the chapter\"),\n",
      "    ResponseSchema(name=\"word_count\", description=\"The word count of the chapter\"),\n",
      "]\n",
      "\n",
      "chapter_output_parser = StructuredOutputParser.from_response_schemas(chapter_response_schemas)\n",
      "\n",
      "# Update the chapter prompt template\n",
      "chapter_prompt_template = ChatPromptTemplate(\n",
      "    messages=[\n",
      "        HumanMessagePromptTemplate.from_template(\n",
      "            \"\"\"This is the outline for the chapter: Title: {chapter_title}.\n",
      "Summary of previous chapters: {previous_summary}.\n",
      "Summary for this chapter: {chapter_summary}.\n",
      "Characters and their associated personalities and behaviors so far: {characters}.\n",
      "\n",
      "Based on this outline, generate a LENGTHY chapter {chapter_number} of AT LEAST 1000 WORD TOKENS.\n",
      "The story should be written in PARAGRAPHS (not bullet points) and fit for a {user_age}-year-old kid in English.\n",
      "Please ensure the chapter has AT LEAST 1000 words and does not include any additional notes.\n",
      "Overall, the total number of tokens should be less than 8000 tokens (all characters included).\n",
      "\n",
      "{format_instructions}\"\"\"\n",
      "        )\n",
      "    ],\n",
      "    input_variables=[\"chapter_title\", \"previous_summary\", \"chapter_summary\", \"characters\", \"chapter_number\", \"user_age\"],\n",
      "    partial_variables={\"format_instructions\": chapter_output_parser.get_format_instructions()}\n",
      ")\n",
      "\n",
      "# Create LLMChain for chapter generation\n",
      "chapter_chain = LLMChain(llm=llm, prompt=chapter_prompt_template)\n",
      "\n",
      "def generate_chapter(chapter_title, previous_summary, chapter_summary, characters, chapter_number, user_age):\n",
      "    chapter = chapter_chain.run(\n",
      "        chapter_title=chapter_title,\n",
      "        previous_summary=previous_summary,\n",
      "        chapter_summary=chapter_summary,\n",
      "        characters=characters,\n",
      "        chapter_number=chapter_number,\n",
      "        user_age=user_age\n",
      "    )\n",
      "    parsed_chapter = chapter_output_parser.parse(chapter)\n",
      "    print(f\"\\nChapter {chapter_number} Generated:\")\n",
      "    print(f\"Word count: {parsed_chapter['word_count']}\")\n",
      "    print(parsed_chapter['content'])\n",
      "    return parsed_chapter\n",
      "\n",
      "# Update the character extraction prompt and parser\n",
      "character_response_schemas = [\n",
      "    ResponseSchema(name=\"characters\", description=\"List of main characters with their descriptions\")\n",
      "]\n",
      "\n",
      "character_output_parser = StructuredOutputParser.from_response_schemas(character_response_schemas)\n",
      "\n",
      "character_extraction_prompt = ChatPromptTemplate(\n",
      "    messages=[\n",
      "        HumanMessagePromptTemplate.from_template(\n",
      "            \"\"\"Extract the main characters from the following story text, along with a brief description of their personalities and behaviors:\n",
      "\n",
      "{story_text}\n",
      "\n",
      "{format_instructions}\n",
      "\n",
      "Format your response as a list of characters, each with a short description:\n",
      "- Character Name: Brief description of personality and behavior\"\"\"\n",
      "        )\n",
      "    ],\n",
      "    input_variables=[\"story_text\"],\n",
      "    partial_variables={\"format_instructions\": character_output_parser.get_format_instructions()}\n",
      ")\n",
      "\n",
      "# Create LLMChain for character extraction\n",
      "character_extraction_chain = LLMChain(llm=llm, prompt=character_extraction_prompt)\n",
      "\n",
      "# Update the extract_characters function\n",
      "def extract_characters(input_story):\n",
      "    characters = character_extraction_chain.run(story_text=input_story)\n",
      "    parsed_characters = character_output_parser.parse(characters)\n",
      "    return parsed_characters['characters']\n",
      "\n",
      "def generate_story_in_chapters(summary_object, user_age):\n",
      "    start_time = time.time()\n",
      "    \n",
      "    # Generate Chapter 1\n",
      "    chapter1 = generate_chapter(\"Chapter 1\", \"\", summary_object['chapter_1'], \"\", 1, user_age)\n",
      "    characters = extract_characters(chapter1['content'])\n",
      "    chapter1_summary = summarizer(chapter1['content'], max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
      "    \n",
      "    # Generate Chapter 2\n",
      "    chapter2 = generate_chapter(\"Chapter 2\", chapter1_summary, summary_object['chapter_2'], characters, 2, user_age)\n",
      "    chapter2_summary = summarizer(chapter2['content'], max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
      "    \n",
      "    # Generate Chapter 3\n",
      "    previous_summary = f\"Chapter 1: {chapter1_summary}\\nChapter 2: {chapter2_summary}\"\n",
      "    chapter3 = generate_chapter(\"Chapter 3\", previous_summary, summary_object['chapter_3'], characters, 3, user_age)\n",
      "    \n",
      "    end_time = time.time()\n",
      "    total_time = end_time - start_time\n",
      "    print(f\"Total time taken: {total_time} seconds\")\n",
      "    \n",
      "    return chapter1, chapter2, chapter3\n",
      "\n",
      "def chat_loop():\n",
      "    while True:\n",
      "        user_age = int(input(\"Enter the age of the child (e.g., 5): \"))\n",
      "        input_from_user = input(\"What would you like the story to be about? \")\n",
      "        \n",
      "        summary_object = generate_summary(input_from_user, user_age)\n",
      "        user_response = input(\"Do you want to continue with this summary? (YES/NO): \")\n",
      "        \n",
      "        while user_response.upper() == \"NO\":\n",
      "            question = input(\"Do you want a new story or make changes to this storyline? (NEW STORY/MODIFY): \")\n",
      "            if question.upper() == \"MODIFY\":          \n",
      "                new_prompt = input(\"Please provide the changes you want to see in this story: \")\n",
      "                summary_prompt = f\"Make the following changes to your story summary, ensuring you maintain complete summaries for all three chapters: {new_prompt}\"\n",
      "                summary_object = generate_summary(summary_prompt, user_age)\n",
      "            elif question.upper() == \"NEW STORY\":\n",
      "                input_from_user = input(\"Let's build you a new story! What would you like to hear a story about? \")\n",
      "                summary_object = generate_summary(input_from_user, user_age)\n",
      "            else:\n",
      "                print(\"Invalid option. Please choose NEW STORY or MODIFY.\")\n",
      "                continue\n",
      "            \n",
      "            user_response = input(\"Do you want to continue with this summary? (YES/NO): \")\n",
      "        \n",
      "        if user_response.upper() == \"YES\":\n",
      "            try:\n",
      "                generated_chapters = generate_story_in_chapters(summary_object, user_age)\n",
      "                print(\"\\nFull story generated successfully!\")\n",
      "                # Here you can add code to save or further process the generated chapters\n",
      "            except Exception as e:\n",
      "                print(f\"An error occurred: {e}\")\n",
      "                print(\"Let's try again with a new story idea.\")\n",
      "                continue\n",
      "        \n",
      "        continue_response = input(\"Do you want to create another story? (YES/NO): \")\n",
      "        if continue_response.upper() != \"YES\":\n",
      "            print(\"Thank you for using the story generator. Goodbye!\")\n",
      "            break\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    chat_loop()\n",
      "import pydantic\n",
      "print(tensorflow.__version__)\n",
      "import pydantic\n",
      "print(tf.__version__)\n",
      "import tensorflow\n",
      "import tensorflow\n",
      "\n",
      "print(tensorflow.version)\n",
      "import tensorflow\n",
      "\n",
      "print(tensorflow.version())\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0392a4fa-a65c-4f10-97b8-62bb15ed2e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 3.6.0\n",
      "Uninstalling keras-3.6.0:\n",
      "  Successfully uninstalled keras-3.6.0\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall keras -y\n",
    "!pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099361c1-2394-44e3-9849-785693cccc46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.18.0rc1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.29.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: keras\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339a87b9-32c2-4876-83ce-f2b5bcc4695c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 19:47:18.662120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1729021638.690922   34581 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1729021638.699150   34581 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 19:47:18.728656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de963eb8-b2db-4c6d-ac41-4e3e77e7ce75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m119"
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
